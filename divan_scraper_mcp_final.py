import os
import psycopg2
import pandas as pd
import re
import logging
import time
from datetime import datetime
from dotenv import load_dotenv

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('divan_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

class DivanScraperMCPFinal:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–∏–≤–∞–Ω–æ–≤ —á–µ—Ä–µ–∑ MCP webscraping"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        self.base_url = "https://www.divan.ru/blagoveshchensk/category/divany"
        self.max_pages = 3  # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.delay_between_requests = 1.0  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        self.db_config = {
            'host': os.getenv('DB_HOST'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'database': os.getenv('DB_NAME'),
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD')
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.validate_config()
        
    def validate_config(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        required_vars = ['DB_HOST', 'DB_NAME', 'DB_USER', 'DB_PASSWORD']
        missing_vars = [var for var in required_vars if not os.getenv(var)]
        
        if missing_vars:
            logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {missing_vars}")
            raise ValueError(f"–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å: {missing_vars}")
        
        logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")
        
    def create_table(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã divans –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç"""
        try:
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS divans (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(500),
                    price DECIMAL(10,2),
                    old_price DECIMAL(10,2),
                    discount_percent INTEGER,
                    dimensions VARCHAR(100),
                    sleeping_dimensions VARCHAR(100),
                    url VARCHAR(500),
                    image_url VARCHAR(500),
                    page_number INTEGER,
                    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """)
            
            conn.commit()
            logger.info("–¢–∞–±–ª–∏—Ü–∞ divans –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
            raise
        finally:
            if conn:
                conn.close()
    
    def get_mcp_data(self, url):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ MCP webscraping
        –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ MCP API
        """
        try:
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å: {url}")
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ MCP
            from mcp_real_data import get_mcp_data_by_page
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ URL
            if "page-" in url:
                page_number = int(url.split("page-")[-1])
            else:
                page_number = 1
                
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            return get_mcp_data_by_page(page_number)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö MCP: {e}")
            return None
    

    
    def validate_mcp_response(self, response):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ MCP"""
        if not response:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        required_elements = ['–¥–∏–≤–∞–Ω', '—Ä—É–±.']
        response_lower = response.lower()
        
        has_required = all(element in response_lower for element in required_elements)
        
        if not has_required:
            logger.warning("–û—Ç–≤–µ—Ç MCP –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–∂–∏–¥–∞–µ–º—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã")
            
        return has_required
    
    def parse_mcp_text_data(self, text_content, page_number=1):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞ MCP webscraping"""
        products = []
        
        if not self.validate_mcp_response(text_content):
            logger.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç MCP –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}")
            return products
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        lines = text_content.split('\n')
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∏–≤–∞–Ω–æ–≤
        product_patterns = [
            # –ü–∞—Ç—Ç–µ—Ä–Ω: [–ù–∞–∑–≤–∞–Ω–∏–µ] (/product/...) —Ü–µ–Ω–∞ —Ä—É–±. —Å—Ç–∞—Ä–∞—è_—Ü–µ–Ω–∞ —Ä—É–±. —Å–∫–∏–¥–∫–∞
            r'\[([^\]]+)\]\s*\(/product/[^)]+\)\s*([\d\s]+)—Ä—É–±\.\s*([\d\s]+)—Ä—É–±\.\s*(\d+)',
            # –ü–∞—Ç—Ç–µ—Ä–Ω: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–Ω–∞ —Ä—É–±. —Å—Ç–∞—Ä–∞—è_—Ü–µ–Ω–∞ —Ä—É–±. —Å–∫–∏–¥–∫–∞
            r'([^[]+)\s*([\d\s]+)—Ä—É–±\.\s*([\d\s]+)—Ä—É–±\.\s*(\d+)',
            # –ü–∞—Ç—Ç–µ—Ä–Ω: –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–Ω–∞ —Ä—É–±. —Å–∫–∏–¥–∫–∞ (–±–µ–∑ —Å—Ç–∞—Ä–æ–π —Ü–µ–Ω—ã)
            r'([^[]+)\s*([\d\s]+)—Ä—É–±\.\s*(\d+)'
        ]
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤
        dimensions_patterns = [
            r'–†–∞–∑–º–µ—Ä—ã \(–î—Ö–®—Ö–í\)\s*:\s*([\d\sx]+)—Å–º',
            r'–†–∞–∑–º–µ—Ä—ã \(–î—Ö–®—Ö–í\)\s*([\d\sx]+)—Å–º',
            r'–†–∞–∑–º–µ—Ä—ã:\s*([\d\sx]+)—Å–º'
        ]
        
        sleeping_patterns = [
            r'–°–ø–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ \(–î—Ö–®—Ö–í\)\s*:\s*([\d\sx]+)—Å–º',
            r'–°–ø–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ \(–î—Ö–®—Ö–í\)\s*([\d\sx]+)—Å–º',
            r'–°–ø–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ:\s*([\d\sx]+)—Å–º'
        ]
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            # –ò—â–µ–º –¥–∏–≤–∞–Ω—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            for pattern in product_patterns:
                matches = re.findall(pattern, line)
                if matches:
                    for match in matches:
                        try:
                            if len(match) == 4:  # –ù–∞–∑–≤–∞–Ω–∏–µ, —Ü–µ–Ω–∞, —Å—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞, —Å–∫–∏–¥–∫–∞
                                name, price, old_price, discount = match
                            elif len(match) == 3:  # –ù–∞–∑–≤–∞–Ω–∏–µ, —Ü–µ–Ω–∞, —Å–∫–∏–¥–∫–∞
                                name, price, discount = match
                                old_price = None
                            else:
                                continue
                            
                            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                            name = name.strip()
                            price = re.sub(r'[^\d]', '', price.strip())
                            if old_price:
                                old_price = re.sub(r'[^\d]', '', old_price.strip())
                            discount = re.sub(r'[^\d]', '', discount.strip())
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –¥–∏–≤–∞–Ω
                            if '–¥–∏–≤–∞–Ω' in name.lower() or '–∫—É—à–µ—Ç–∫–∞' in name.lower():
                                # –ò—â–µ–º —Ä–∞–∑–º–µ—Ä—ã –≤ –±–ª–∏–∂–∞–π—à–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö
                                dimensions = self.find_dimensions(lines, i, dimensions_patterns)
                                sleeping_dimensions = self.find_dimensions(lines, i, sleeping_patterns)
                                
                                # –§–æ—Ä–º–∏—Ä—É–µ–º URL –ø—Ä–æ–¥—É–∫—Ç–∞
                                url = f"https://www.divan.ru/product/{name.lower().replace(' ', '-')}"
                                
                                product = {
                                    'name': name,
                                    'price': float(price) if price else None,
                                    'old_price': float(old_price) if old_price else None,
                                    'discount_percent': int(discount) if discount else None,
                                    'dimensions': dimensions,
                                    'sleeping_dimensions': sleeping_dimensions,
                                    'url': url,
                                    'image_url': None,
                                    'page_number': page_number
                                }
                                
                                products.append(product)
                                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥–∏–≤–∞–Ω: {name[:50]}... - {price}‚ÇΩ (—Å—Ç—Ä. {page_number})")
                                
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–æ–∫–∏: {e}")
                            continue
        
        return products
    
    def find_dimensions(self, lines, current_index, patterns):
        """–ü–æ–∏—Å–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤ –±–ª–∏–∂–∞–π—à–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö"""
        # –ò—â–µ–º —Ä–∞–∑–º–µ—Ä—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ ¬±5 —Å—Ç—Ä–æ–∫
        for j in range(max(0, current_index-5), min(len(lines), current_index+5)):
            for pattern in patterns:
                match = re.search(pattern, lines[j])
                if match:
                    return match.group(1).strip()
        return None
    
    def save_to_database(self, products):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if not products:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return 0
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cursor = conn.cursor()
            
            saved_count = 0
            for product in products:
                try:
                    cursor.execute("""
                        INSERT INTO divans (name, price, old_price, discount_percent, 
                                         dimensions, sleeping_dimensions, url, image_url, page_number)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (name) DO UPDATE SET
                            price = EXCLUDED.price,
                            old_price = EXCLUDED.old_price,
                            discount_percent = EXCLUDED.discount_percent,
                            dimensions = EXCLUDED.dimensions,
                            sleeping_dimensions = EXCLUDED.sleeping_dimensions,
                            page_number = EXCLUDED.page_number,
                            scraped_at = CURRENT_TIMESTAMP
                    """, (
                        product['name'],
                        product['price'],
                        product['old_price'],
                        product['discount_percent'],
                        product['dimensions'],
                        product['sleeping_dimensions'],
                        product['url'],
                        product['image_url'],
                        product['page_number']
                    ))
                    saved_count += 1
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {product['name']}: {e}")
                    continue
            
            conn.commit()
            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö: {saved_count} –¥–∏–≤–∞–Ω–æ–≤")
            return saved_count
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—É: {e}")
            return 0
        finally:
            if conn:
                conn.close()
    
    def scrape_all_pages(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞"""
        all_products = []
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {self.max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü...")
        
        for page in range(1, self.max_pages + 1):
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if page == 1:
                    url = self.base_url
                else:
                    url = f"{self.base_url}/page-{page}"
                
                logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}/{self.max_pages}: {url}")
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ MCP
                page_data = self.get_mcp_data(url)
                
                if page_data:
                    # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    products = self.parse_mcp_text_data(page_data, page)
                    all_products.extend(products)
                    
                    logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(products)} –¥–∏–≤–∞–Ω–æ–≤")
                else:
                    logger.warning(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if page < self.max_pages:
                    time.sleep(self.delay_between_requests)
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
                continue
        
        logger.info(f"üéØ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –¥–∏–≤–∞–Ω–æ–≤: {len(all_products)}")
        return all_products
    
    def export_to_csv(self, filename="divans_mcp_final.csv"):
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–∞–π–ª"""
        try:
            conn = psycopg2.connect(**self.db_config)
            
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã
            df = pd.read_sql_query("""
                SELECT name, price, old_price, discount_percent, 
                       dimensions, sleeping_dimensions, url, page_number, scraped_at
                FROM divans 
                ORDER BY scraped_at DESC
            """, conn)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")
            logger.info(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if not df.empty:
                self.show_statistics(df)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ: {e}")
        finally:
            if conn:
                conn.close()
    
    def show_statistics(self, df):
        """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–∞–Ω–Ω—ã–º"""
        print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"üí∞ –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: {df['price'].mean():,.0f}‚ÇΩ")
        print(f"üí∞ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {df['price'].min():,.0f}‚ÇΩ")
        print(f"üí∞ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {df['price'].max():,.0f}‚ÇΩ")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–∫–∏–¥–∫–∏ (—É–±–∏—Ä–∞–µ–º NaN)
        valid_discounts = df['discount_percent'].dropna()
        if len(valid_discounts) > 0:
            print(f"üéØ –°—Ä–µ–¥–Ω—è—è —Å–∫–∏–¥–∫–∞: {valid_discounts.mean():.1f}%")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {df['page_number'].nunique()}")
        
        # –¢–æ–ø-5 —Å–∞–º—ã—Ö –¥–æ—Ä–æ–≥–∏—Ö –¥–∏–≤–∞–Ω–æ–≤
        print("\nüèÜ –¢–û–ü-5 —Å–∞–º—ã—Ö –¥–æ—Ä–æ–≥–∏—Ö –¥–∏–≤–∞–Ω–æ–≤:")
        top_expensive = df.nlargest(5, 'price')[['name', 'price', 'discount_percent', 'page_number']]
        for _, row in top_expensive.iterrows():
            discount_str = f"{row['discount_percent']}%" if pd.notna(row['discount_percent']) else "N/A"
            print(f"   ‚Ä¢ {row['name'][:40]}... - {row['price']:,.0f}‚ÇΩ (—Å–∫–∏–¥–∫–∞: {discount_str}, —Å—Ç—Ä. {row['page_number']})")
    
    def run_scraping(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –¥–∏–≤–∞–Ω–æ–≤ —á–µ—Ä–µ–∑ MCP webscraping...")
        logger.info(f"üåê –ë–∞–∑–æ–≤—ã–π URL: {self.base_url}")
        logger.info(f"üìÑ –ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {self.max_pages}")
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
            self.create_table()
            
            # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            all_products = self.scrape_all_pages()
            
            if all_products:
                logger.info(f"üì¶ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –¥–∏–≤–∞–Ω–æ–≤: {len(all_products)}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
                saved_count = self.save_to_database(all_products)
                
                if saved_count > 0:
                    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ CSV
                    self.export_to_csv()
                    
                    logger.info(f"üéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –¥–∏–≤–∞–Ω–æ–≤")
                else:
                    logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑—É")
            else:
                logger.warning("‚ö†Ô∏è –î–∏–≤–∞–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö")
                
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
            raise
        
        logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        scraper = DivanScraperMCPFinal()
        scraper.run_scraping()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: {e}")
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª–µ divan_scraper.log")

if __name__ == "__main__":
    main()
